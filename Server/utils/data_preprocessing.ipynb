{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model1 = load_model('../models/best_age_model.keras')\n",
    "model2 = load_model('../models/best_emotion_model.keras')\n",
    "model3 = load_model('../models/best_gender_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained face detector (e.g., Haar cascades)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender label dictionary\n",
    "gender_dict = {0: 'Male \\u2642', 1: 'Female \\u2640'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion label dictionary with emojis\n",
    "emotion_labels = {\n",
    "    'Angry': ('Angry 😠', (255, 0, 0)),  # Red\n",
    "    'Happy': ('Happy 😃', (0, 255, 0)),  # Green\n",
    "    'Neutral': ('Neutral 😐', (255, 255, 255)),  # White\n",
    "    'Sad': ('Sad 😢', (0, 0, 255)),  # Blue\n",
    "    'Surprise': ('Surprised 😲', (255, 255, 0))  # Yellow\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the image for prediction\n",
    "def preprocess_image(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        (x, y, w, h) = faces[0]  # Assuming the first detected face is the target\n",
    "        face = gray[y:y+h, x:x+w]  # Crop the face\n",
    "    else:\n",
    "        face = gray  # Use the whole frame if no face is detected\n",
    "    \n",
    "    face = cv2.resize(face, (128, 128))  # Resize to model input size\n",
    "    face = face / 255.0  # Normalize\n",
    "    face = np.expand_dims(face, axis=-1)  # Add channel dimension\n",
    "    face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
    "    \n",
    "    return face, (x, y, w, h) if len(faces) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions for gender, age, and emotion\n",
    "def predict_gender_age_emotion(img):\n",
    "    processed_img, face_bbox = preprocess_image(img)\n",
    "\n",
    "    # Predictions from the three models\n",
    "    pred_age = model1.predict(processed_img)\n",
    "    pred_emotion = model2.predict(processed_img)\n",
    "    pred_gender = model3.predict(processed_img)\n",
    "    \n",
    "    # Process gender prediction\n",
    "    pred_gender_prob = pred_gender[0][0].item()  # Extract scalar using item()\n",
    "    pred_gender_label = gender_dict[round(pred_gender_prob)]  # Round to nearest gender\n",
    "    \n",
    "    # Process age prediction\n",
    "    pred_age_value = pred_age[0][0].item()  # Extract scalar using item()\n",
    "    range_width = max(2, int(0.1 * pred_age_value))  # 10% of the predicted age, with a minimum range of 2\n",
    "    pred_age_lower = max(0, round(pred_age_value - range_width))  # Lower bound\n",
    "    pred_age_upper = round(pred_age_value + range_width)  # Upper bound\n",
    "    \n",
    "    # Process emotion prediction\n",
    "    emotion_label = list(emotion_labels.keys())[np.argmax(pred_emotion)]\n",
    "    pred_emotion_label, emotion_color = emotion_labels[emotion_label]  # Get the label with emoji and its color\n",
    "    \n",
    "    return pred_gender_label, (pred_age_lower, pred_age_upper), pred_emotion_label, emotion_color, face_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply blur effect on the background behind the text\n",
    "def apply_blur(frame, background_position):\n",
    "    # Convert OpenCV image to Pillow Image\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Extract the region where the blur will be applied\n",
    "    region = pil_image.crop(background_position)\n",
    "    \n",
    "    # Apply Gaussian blur to the region\n",
    "    blurred_region = region.filter(ImageFilter.GaussianBlur(radius=20))\n",
    "    \n",
    "    # Paste the blurred region back onto the image\n",
    "    pil_image.paste(blurred_region, background_position)\n",
    "    \n",
    "    # Convert back to OpenCV\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Segoe UI Emoji font (on Windows)\n",
    "font_path = 'C:/Windows/Fonts/seguiemj.ttf'\n",
    "\n",
    "# Function to draw text with Pillow and apply blur background effect\n",
    "def draw_text_with_pillow(frame, text, position, font_path=font_path, font_size=30, text_color=(255, 255, 255), padding=10):\n",
    "    # Convert OpenCV image to Pillow Image\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    # Load a font with emoji support\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "    # Calculate text size and add padding\n",
    "    text_bbox = draw.textbbox(position, text, font=font)\n",
    "    text_size = (text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1])\n",
    "    \n",
    "    # Define the background position with padding\n",
    "    background_position = [position[0] - padding, position[1] - padding, position[0] + text_size[0] + padding, position[1] + text_size[1] + padding]\n",
    "    \n",
    "    # Apply the blur effect to the region behind the text\n",
    "    frame = apply_blur(frame, background_position)\n",
    "    \n",
    "    # Draw the text with Pillow\n",
    "    draw.text(position, text, font=font, fill=text_color)  # Colored text\n",
    "    \n",
    "    # Convert Pillow Image back to OpenCV\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Predict gender, age, and emotion\n",
    "    pred_gender, (pred_age_lower, pred_age_upper), pred_emotion, emotion_color, face_bbox = predict_gender_age_emotion(frame)\n",
    "    \n",
    "    # If a face is detected, draw a bounding box around the face\n",
    "    if face_bbox is not None:\n",
    "        (x, y, w, h) = face_bbox\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the predictions using Pillow with better spacing and padding\n",
    "        frame = draw_text_with_pillow(frame, f'Emotion: {pred_emotion}', (x, y - 120), text_color=emotion_color, font_size=30)\n",
    "        frame = draw_text_with_pillow(frame, f'Gender: {pred_gender}', (x, y - 80), text_color=(255, 165, 0), font_size=28)\n",
    "        frame = draw_text_with_pillow(frame, f'Age: {pred_age_lower}-{pred_age_upper}', (x, y - 40), text_color=(255,250,205), font_size=28)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Gender, Age, and Emotion Prediction', frame)\n",
    "    \n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When everything is done, release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
