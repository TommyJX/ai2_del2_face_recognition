{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import keyboard\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained models\n",
    "model1 = load_model('../models/best_age_model.keras')\n",
    "model2 = load_model('../models/best_emotion_model.keras')\n",
    "model3 = load_model('../models/best_gender_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe face detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender label dictionary\n",
    "gender_dict = {0: 'Male \\u2642', 1: 'Female \\u2640'}\n",
    "\n",
    "# Emotion label dictionary with emojis\n",
    "emotion_labels = {\n",
    "    'Angry': ('Angry üò†', (255, 0, 0)),  # Red\n",
    "    'Happy': ('Happy üòÉ', (0, 255, 0)),  # Green\n",
    "    'Neutral': ('Neutral üòê', (255, 255, 255)),  # White\n",
    "    'Sad': ('Sad üò¢', (0, 0, 255)),  # Blue\n",
    "    'Surprise': ('Surprised üò≤', (255, 255, 0))  # Yellow\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the image for prediction\n",
    "def preprocess_image(img, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    face = img[y:y+h, x:x+w]  # Crop the face\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    face = cv2.resize(face, (128, 128))  # Resize to model input size\n",
    "    face = face / 255.0  # Normalize\n",
    "    face = np.expand_dims(face, axis=-1)  # Add channel dimension\n",
    "    face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions for gender, age, and emotion\n",
    "def predict_gender_age_emotion(img, bbox):\n",
    "    face_img = preprocess_image(img, bbox)\n",
    "\n",
    "    # Predictions from the three models\n",
    "    pred_age = model1.predict(face_img)\n",
    "    pred_emotion = model2.predict(face_img)\n",
    "    pred_gender = model3.predict(face_img)\n",
    "    \n",
    "    # Process gender prediction\n",
    "    pred_gender_prob = pred_gender[0][0].item()  # Extract scalar using item()\n",
    "    pred_gender_label = gender_dict[round(pred_gender_prob)]  # Round to nearest gender\n",
    "    \n",
    "    # Process age prediction\n",
    "    pred_age_value = pred_age[0][0].item()  # Extract scalar using item()\n",
    "    range_width = max(2, int(0.1 * pred_age_value))  # 10% of the predicted age, with a minimum range of 2\n",
    "    pred_age_lower = max(0, round(pred_age_value - range_width))  # Lower bound\n",
    "    pred_age_upper = round(pred_age_value + range_width)  # Upper bound\n",
    "    \n",
    "    # Process emotion prediction\n",
    "    emotion_label = list(emotion_labels.keys())[np.argmax(pred_emotion)]\n",
    "    pred_emotion_label, emotion_color = emotion_labels[emotion_label]  # Get the label with emoji and its color\n",
    "    \n",
    "    return pred_gender_label, (pred_age_lower, pred_age_upper), pred_emotion_label, emotion_color, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Segoe UI Emoji font (on Windows)\n",
    "font_path = 'C:/Windows/Fonts/seguiemj.ttf'\n",
    "\n",
    "# Function to draw text with Pillow\n",
    "def draw_text_with_pillow(frame, text, position, font_path=font_path, font_size=30, text_color=(255, 255, 255), padding=10):\n",
    "    # Convert OpenCV image to Pillow Image\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    # Load a font with emoji support\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "    # Draw the text with Pillow\n",
    "    draw.text(position, text, font=font, fill=text_color)  # Colored text\n",
    "    \n",
    "    # Convert Pillow Image back to OpenCV\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the MediaPipe face detection model\n",
    "with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the image color format from BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Perform face detection using MediaPipe\n",
    "        results = face_detection.process(rgb_frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Get bounding box of the detected face\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "                # Make predictions using the cropped face\n",
    "                pred_gender, (pred_age_lower, pred_age_upper), pred_emotion, emotion_color, bbox = predict_gender_age_emotion(frame, (x, y, w, h))\n",
    "                \n",
    "                # Draw a rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw the predictions using Pillow\n",
    "                frame = draw_text_with_pillow(frame, f'Emotion: {pred_emotion}', (x, y - 120), text_color=emotion_color, font_size=30)\n",
    "                frame = draw_text_with_pillow(frame, f'Gender: {pred_gender}', (x, y - 80), text_color=(255, 165, 0), font_size=28)\n",
    "                frame = draw_text_with_pillow(frame, f'Age: {pred_age_lower}-{pred_age_upper}', (x, y - 40), text_color=(255, 250, 205), font_size=28)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Gender, Age, and Emotion Prediction', frame)\n",
    "\n",
    "        if keyboard.is_pressed('ctrl+q'):\n",
    "            print(\"Ctrl + Q pressed, exiting...\")\n",
    "            break\n",
    "        \n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When everything is done, release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
