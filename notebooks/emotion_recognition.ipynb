{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "BASE_DIR = '../dataset/emotion-recog-dataset'\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = ['Angry', 'Happy', 'Neutral', 'Sad', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image paths and labels\n",
    "image_paths = []\n",
    "emotion_classes = []\n",
    "\n",
    "for emotion in emotion_labels:\n",
    "    emotion_dir = os.path.join(BASE_DIR, emotion)\n",
    "    for filename in tqdm(os.listdir(emotion_dir)):\n",
    "        image_path = os.path.join(emotion_dir, filename)\n",
    "        image_paths.append(image_path)\n",
    "        emotion_classes.append(emotion_labels.index(emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'image': image_paths,\n",
    "    'emotion': emotion_classes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map emotion labels to their names\n",
    "emotion_dict = {i: emotion for i, emotion in enumerate(emotion_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image with its label\n",
    "img = Image.open(df['image'][0])\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Emotion: {emotion_dict[df['emotion'][0]]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of emotions\n",
    "sns.countplot(x='emotion', data=df, palette='Set2')\n",
    "plt.title(\"Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a grid of images with labels\n",
    "plt.figure(figsize=(20, 20))\n",
    "files = df.iloc[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Adjust the spacing between images and titles\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2)  # hspace controls height space, wspace controls width space\n",
    "\n",
    "for index, row in enumerate(files.itertuples(index=False, name=None), 1):\n",
    "    plt.subplot(5, 5, index)\n",
    "    \n",
    "    file = row[0]\n",
    "    emotion = row[1]\n",
    "    \n",
    "    img = load_img(file)\n",
    "    img = np.array(img)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Emotion: {emotion_dict[emotion]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from images\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, color_mode='grayscale')\n",
    "        img = img.resize((128, 128), Image.Resampling.LANCZOS) \n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features), 128, 128, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(df['image'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "y_emotion = to_categorical(df['emotion'], num_classes=len(emotion_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model input shape\n",
    "input_shape = (128, 128, 1)\n",
    "inputs = Input((input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convolutional layers\n",
    "conv_1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "maxp_1 = MaxPooling2D(pool_size=(2, 2))(conv_1)\n",
    "dropout_conv_1 = Dropout(0.5)(maxp_1)  # Add dropout in conv layer\n",
    "\n",
    "conv_2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(dropout_conv_1)\n",
    "maxp_2 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n",
    "dropout_conv_2 = Dropout(0.5)(maxp_2)  # Add dropout in conv layer\n",
    "\n",
    "conv_3 = Conv2D(128, kernel_size=(3, 3), activation='relu')(dropout_conv_2)\n",
    "maxp_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n",
    "dropout_conv_3 = Dropout(0.5)(maxp_3)  # Add dropout in conv layer\n",
    "\n",
    "conv_4 = Conv2D(256, kernel_size=(3, 3), activation='relu')(dropout_conv_3)\n",
    "maxp_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)\n",
    "dropout_conv_4 = Dropout(0.5)(maxp_4)  # Add dropout in conv layer\n",
    "\n",
    "flatten = Flatten()(dropout_conv_4)\n",
    "\n",
    "# Fully connected layers with L2 regularization\n",
    "dense_1 = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(flatten)\n",
    "dense_2 = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(flatten)\n",
    "\n",
    "dropout_1 = Dropout(0.6)(dense_1)\n",
    "dropout_2 = Dropout(0.6)(dense_2)\n",
    "\n",
    "# Output layer for emotion classification\n",
    "output_1 = Dense(len(emotion_labels), activation='softmax', name='emotion_out')(dropout_2)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output_1])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7),\n",
    "    ModelCheckpoint('../models/best_emotion_model.keras', monitor='val_loss', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x=X,\n",
    "    y=y_emotion,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X, y_emotion)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set used during training\n",
    "val_loss, val_accuracy = model.evaluate(X[int(0.8*len(X)):], y_emotion[int(0.8*len(y_emotion)):])\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "# Create epochs range\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of predicting emotion from a specific image\n",
    "image_index = 100  # Just an example index\n",
    "print(\"Original Emotion: \", emotion_dict[df['emotion'][image_index]])\n",
    "\n",
    "# Predict from model\n",
    "pred = model.predict(X[image_index].reshape(1, 128, 128, 1))\n",
    "pred_emotion = emotion_dict[np.argmax(pred)]\n",
    "\n",
    "print(\"Predicted Emotion: \", pred_emotion)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(X[image_index].reshape(128, 128), cmap='gray');\n",
    "plt.title(f\"Predicted Emotion: {pred_emotion}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
