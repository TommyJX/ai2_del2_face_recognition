{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_path = os.path.abspath('../dataset/emotion-recog-dataset')\n",
    "emotion_labels = ['Angry', 'Happy', 'Neutral', 'Sad', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_emotion_data(dataset_path, emotion_labels, img_size=(128, 128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in emotion_labels:\n",
    "        emotion_path = os.path.join(dataset_path, label)\n",
    "        print(f\"Looking for images in: {emotion_path}\")  # Debug print\n",
    "        if not os.path.exists(emotion_path):\n",
    "            print(f\"Directory does not exist: {emotion_path}\")\n",
    "            continue\n",
    "        for filename in os.listdir(emotion_path):\n",
    "            img_path = os.path.join(emotion_path, filename)\n",
    "            print(f\"Processing image: {img_path}\")  # Debug print\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "            if img is None:\n",
    "                print(f\"Failed to read image: {img_path}\")\n",
    "                continue\n",
    "            img = cv2.resize(img, img_size)  # Resize to (128, 128)\n",
    "            images.append(img)\n",
    "            labels.append(emotion_labels.index(label))  # Assign a numerical label based on the emotion\n",
    "            \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Normalize the images\n",
    "    images = images / 255.0\n",
    "    \n",
    "    # Reshape images to add channel dimension\n",
    "    images = images.reshape(-1, img_size[0], img_size[1], 1)\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    labels = to_categorical(labels, num_classes=len(emotion_labels))\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y = load_emotion_data(dataset_path, emotion_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "input_shape = (128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)  # Add dropout after first pooling layer\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)  # Add dropout after second pooling layer\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)  # Add dropout after third pooling layer\n",
    "\n",
    "x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)  # Add dropout after fourth pooling layer\n",
    "\n",
    "x = Conv2D(512, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Increase dropout rate\n",
    "outputs = Dense(len(emotion_labels), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        return lr * 0.5\n",
    "    elif epoch > 30:\n",
    "        return lr * 0.75\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Updated Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7),\n",
    "    ModelCheckpoint('../models/best_emotion_model.keras', monitor='val_loss', save_best_only=True),\n",
    "    lr_scheduler\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "# Create epochs range\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
